{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTXAtYCM/6dcNPnCNOv4M9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahadikprasad15/ARENA/blob/main/GPT_2_small_induction_heads%2C_previous_token_heads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdOvPGcAhbmP"
      },
      "outputs": [],
      "source": [
        "%pip install transformer_lens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformer_lens import HookedTransformer"
      ],
      "metadata": {
        "id": "aZbA5Gcphhpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = HookedTransformer.from_pretrained('gpt2-small', device = device)"
      ],
      "metadata": {
        "id": "Q5lMa0tMi_nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "deIewSxll61N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def induction_prompt_generator(batch = 0, seq_length = 10):\n",
        "  vocab_size = model.cfg.d_vocab\n",
        "  tokens = torch.randint(1000, vocab_size, (batch,seq_length))\n",
        "  bos = torch.tensor(model.to_single_token('<|endoftext|>')).repeat(batch).unsqueeze(dim = 1)\n",
        "  combined_seq = torch.cat((bos, tokens), dim=1)\n",
        "  combined_repeat = torch.cat((combined_seq, tokens), dim = 1)\n",
        "\n",
        "  return combined_repeat"
      ],
      "metadata": {
        "id": "vVjBXdcdmFBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = induction_prompt_generator(4,10)\n",
        "logits, cache = model.run_with_cache(prompts)"
      ],
      "metadata": {
        "id": "NVNFS2sKnNKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Limit the number of layers and heads to plot for faster execution\n",
        "max_layers_to_plot = 2  # Plot only the first 2 layers\n",
        "max_heads_to_plot = 4   # Plot only the first 4 heads per layer\n",
        "\n",
        "\n",
        "for layer in range(min(model.cfg.n_layers, max_layers_to_plot)):\n",
        "  for head in range(min(model.cfg.n_heads, max_heads_to_plot)):\n",
        "    attention_pattern = cache[f'blocks.{layer}.attn.hook_pattern'][0, head]\n",
        "    fig = px.imshow(attention_pattern.detach().cpu().numpy(),\n",
        "                    title=f\"Attention Pattern: Layer {layer}, Head {head}\",\n",
        "                    color_continuous_scale='dense') # Added title for clarity\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "zSQ5kMCEnU4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_pattern = cache[f'blocks.3.attn.hook_pattern'][0, 0]\n",
        "fig = px.imshow(attention_pattern.detach().cpu().numpy(),\n",
        "                    title=f\"Attention Pattern: Layer 3, Head 0\",\n",
        "                    color_continuous_scale='dense') # Added title for clarity\n",
        "fig.show()\n",
        "\n",
        "\n",
        "attention_pattern = cache[f'blocks.0.attn.hook_pattern'][0, 5]\n",
        "fig = px.imshow(attention_pattern.detach().cpu().numpy(),\n",
        "                    title=f\"Attention Pattern: Layer 0, Head 5\",\n",
        "                    color_continuous_scale='dense') # Added title for clarity\n",
        "fig.show()\n",
        "\n",
        "attention_pattern = cache[f'blocks.1.attn.hook_pattern'][0, 11]\n",
        "fig = px.imshow(attention_pattern.detach().cpu().numpy(),\n",
        "                    title=f\"Attention Pattern: Layer 1, Head 11\",\n",
        "                    color_continuous_scale='dense') # Added title for clarity\n",
        "fig.show()\n",
        "\n",
        "\n",
        "attention_pattern = cache[f'blocks.0.attn.hook_pattern'][0, 1]\n",
        "fig = px.imshow(attention_pattern.detach().cpu().numpy(),\n",
        "                    title=f\"Attention Pattern: Layer 0, Head 1\",\n",
        "                    color_continuous_scale='dense') # Added title for clarity\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "vcgQmvwUFT5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_pattern = cache[f'blocks.0.attn.hook_pattern'][0,1]\n",
        "\n",
        "def induction_score(cache):\n",
        "  attention_score_tensor = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n",
        "  attention_scores_dict = {}\n",
        "  for layer in range(model.cfg.n_layers):\n",
        "    for head in range(model.cfg.n_heads):\n",
        "      attention_score = cache[f'blocks.{layer}.attn.hook_pattern'][:,head].diagonal(offset = -10, dim1 = 1, dim2 = 2).sum()\n",
        "      attention_scores_dict[layer, head] = attention_score\n",
        "      attention_score_tensor[layer, head] = attention_score\n",
        "\n",
        "  return attention_scores_dict, attention_score_tensor\n",
        "\n",
        "\n",
        "scores_dict, scores_tensor = induction_score(cache)\n",
        "sorted_scores = sorted(scores_dict.items(), key = lambda x: x[1], reverse=True )[:10]\n",
        "\n",
        "print('Top 10 induction scores:')\n",
        "print('\\n')\n",
        "for x,y in sorted_scores:\n",
        "  print(f'Layer {x[0]}, Head {x[1]} --- Score: {y.item():.2f}')"
      ],
      "metadata": {
        "id": "97KwnEEonZgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Induction Head Scores\n",
        "\n",
        "fig = px.imshow(\n",
        "    scores_tensor,\n",
        "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
        "    title=\"Induction Scores by Head\",\n",
        "    text_auto=\".2f\",\n",
        "    color_continuous_scale='dense',\n",
        "    width=1900,\n",
        "    height=700,\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "0dWkNF_2o2ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from  functools import partial"
      ],
      "metadata": {
        "id": "YP1HjzfDRPgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23990e48"
      },
      "source": [
        "def zero_ablation(attention, hook, head):\n",
        "  attention[:, :, head, :] = 0\n",
        "  return attention\n",
        "\n",
        "\n",
        "def mean_ablation(attention, hook, head):\n",
        "  attention[:, :, head, :] = attention[:, :, head, :].mean()\n",
        "  return attention\n",
        "\n",
        "# Baseline logprobs\n",
        "\n",
        "prompts = induction_prompt_generator(5,20)\n",
        "baseline_logits, baseline_cache = model.run_with_cache(prompts)\n",
        "\n",
        "def log_probs_calculate(logits):\n",
        "  probs = torch.nn.functional.log_softmax(logits, dim = -1)\n",
        "  baseline_scores = []\n",
        "  for i in range(5):\n",
        "    for j in range(21, 41):\n",
        "      baseline_scores.append(probs[i,j,prompts[i,j].item()].item())\n",
        "\n",
        "  average_score = torch.tensor(baseline_scores).mean()\n",
        "  return average_score.item()\n",
        "\n",
        "\n",
        "baseline_logprobs = log_probs_calculate(baseline_logits)\n",
        "\n",
        "print(f'The baseline logprobs on correct tokens: {baseline_logprobs}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ablated_logprobs = {}\n",
        "\n",
        "for (layer, head), _ in sorted_scores:\n",
        "  hook_function = partial(zero_ablation, head = head)\n",
        "  zero_ablated_logits = model.run_with_hooks(prompts, fwd_hooks = [(f'blocks.{layer}.attn.hook_z', hook_function)])\n",
        "  ablated_logprobs[layer, head] = zero_ablated_logits\n"
      ],
      "metadata": {
        "id": "6Rir6O9xVQko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "difference = {}\n",
        "for heads, logits in ablated_logprobs.items():\n",
        "  diff = logits - baseline_logprobs\n",
        "  difference[heads] = diff\n",
        "\n"
      ],
      "metadata": {
        "id": "K9Kn-HWbO9BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prev_token_attention_score(cache):\n",
        "  attention_scores = {}\n",
        "  attention_score_tensor = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n",
        "  for layer in range(model.cfg.n_layers):\n",
        "    for head in range(model.cfg.n_heads):\n",
        "      attention_pattern = cache[f'blocks.{layer}.attn.hook_pattern'][: ,head, :, :]\n",
        "      score = attention_pattern.diagonal(dim1 = -2, dim2 = -1, offset = -1).mean()\n",
        "      attention_scores[layer, head] = score\n",
        "      attention_score_tensor[layer, head] = score\n",
        "  return attention_scores, attention_score_tensor\n"
      ],
      "metadata": {
        "id": "1mrlNOTZLbXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Previous token heads\n",
        "score_dict, score_tensor = prev_token_attention_score(cache)\n",
        "\n",
        "prev_token_heads = sorted(score_dict.items(), key = lambda x: x[1], reverse = True)[:5]\n",
        "prev_token_heads"
      ],
      "metadata": {
        "id": "EsmLTlt1MD0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Previous Token Head Scores\n",
        "\n",
        "fig = px.imshow(\n",
        "    score_tensor,\n",
        "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
        "    title=\"Previous Token Scores by Head\",\n",
        "    text_auto=\".2f\",\n",
        "    color_continuous_scale='dense',\n",
        "    width=1900,\n",
        "    height=700,\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "ShcqUW05o7jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformer_lens import FactoredMatrix\n",
        "\n",
        "head_index = 0\n",
        "layer = 3\n",
        "\n",
        "W_O = model.W_O[layer, head_index]\n",
        "W_V = model.W_V[layer, head_index]\n",
        "W_E = model.W_E\n",
        "W_U = model.W_U\n",
        "\n",
        "OV_circuit = FactoredMatrix(W_V, W_O)\n",
        "full_OV_circuit = W_E @ OV_circuit @ W_U"
      ],
      "metadata": {
        "id": "G-3RH-japD2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = torch.randint(0, model.cfg.d_vocab, (200,))\n",
        "full_OV_circuit_sample = full_OV_circuit[indices, indices].AB\n",
        "\n",
        "px.imshow(\n",
        "    full_OV_circuit_sample.detach().cpu().numpy(),\n",
        "    labels={\"x\": \"Logits on output token\", \"y\": \"Input token\"},\n",
        "    title=\"Full OV circuit for copying head\",\n",
        "    width=700,\n",
        "    height=600,\n",
        "    color_continuous_scale= 'dense'\n",
        ")"
      ],
      "metadata": {
        "id": "LhmDC7sLpNNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_1_acc(full_OV_circuit: FactoredMatrix, batch_size: int = 1000) -> float:\n",
        "    \"\"\"\n",
        "    Return the fraction of the time that the maximum value is on the circuit diagonal.\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "\n",
        "    for indices in torch.split(torch.arange(full_OV_circuit.shape[0], device=device), batch_size):\n",
        "        AB_slice = full_OV_circuit[indices].AB\n",
        "        total += (torch.argmax(AB_slice, dim=1) == indices).float().sum().item()\n",
        "\n",
        "    return total / full_OV_circuit.shape[0]\n",
        "\n",
        "\n",
        "print(f\"Fraction of time that the best logit is on diagonal: {top_1_acc(full_OV_circuit):.4f}\")"
      ],
      "metadata": {
        "id": "nyVwyrkVpPTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EhHYD4X8pj3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Awr_cfocplJA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}