{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl5hnHlv5Zko4RPQIBH1tt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahadikprasad15/ARENA/blob/main/Steering_on_GPT2_small.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJnAzr3lWq_X"
      },
      "outputs": [],
      "source": [
        "%pip install transformer_lens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformer_lens\n",
        "import torch\n",
        "import plotly.express as px\n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "gzJ7oasIXj9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = transformer_lens.HookedTransformer.from_pretrained('gpt2-small')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "-bdLbwnyXj6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = model.cfg.n_layers\n",
        "heads = model.cfg.n_heads"
      ],
      "metadata": {
        "id": "ny4xaMfYXj4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,love_cache = model.run_with_cache( 'Love')\n",
        "_,hate_cache = model.run_with_cache( 'Hate')"
      ],
      "metadata": {
        "id": "2uS1qvi6Xj1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I want to take a prompt and steer it using the difference between the love and hate activation vectors, and do it first for a random layer, head, and then systematically for all of them.\n",
        "\n",
        "First we want some baseline, and then change, and plot that for all the heads - and we'll get a list of heads that are important\n",
        "\n"
      ],
      "metadata": {
        "id": "-s-Q_VxtY-zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'I went to the football match because'"
      ],
      "metadata": {
        "id": "F4_Cm2udYbLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_tokens = model.to_tokens(prompt)\n",
        "num_tokens = 10\n",
        "\n",
        "def autoregressive_generator(prompt_tokens, num_tokens):\n",
        "\n",
        "  for n in range(num_tokens):\n",
        "    final_token_pred = model(prompt_tokens).argmax(dim = -1)[:, -1]\n",
        "    prompt_tokens = torch.cat([prompt_tokens, final_token_pred.unsqueeze(0)], dim = -1)\n",
        "\n",
        "  return prompt_tokens"
      ],
      "metadata": {
        "id": "fzrhUvY5YrwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to_string(autoregressive_generator(prompt_tokens, 5))"
      ],
      "metadata": {
        "id": "hDRZQ-tlhYOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, I now know how to generate the few tokens. This can be used in run_with_hooks.\n",
        "So, now I need to get an activation from a head, add it to the same head as hook intervention and keep generating output of that for a few tokens.\n",
        "\n",
        "Output of the run_with_hooks will have logits, we just take argmax for the last position, add it to the prompt and keep doing it for a few tokens."
      ],
      "metadata": {
        "id": "7DaAchpRck84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "love_activation_full = love_cache['blocks.5.hook_resid_post']\n",
        "hate_activation_full = hate_cache['blocks.5.hook_resid_post']\n",
        "\n",
        "# Extract the activation for the last token from each before subtraction\n",
        "love_last_token_activation = love_activation_full[:, -1, :]\n",
        "hate_last_token_activation = hate_activation_full[:, -1, :]\n",
        "\n",
        "# Calculate the difference for steering\n",
        "steering_activation = love_last_token_activation - hate_last_token_activation\n",
        "steering_activation = steering_activation.squeeze(0) # Remove the batch dimension if present"
      ],
      "metadata": {
        "id": "I4DEHJBPZ73j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = 5"
      ],
      "metadata": {
        "id": "F1tOWv-5Z039"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hook_function(activation, hook, alpha):\n",
        "  activation[:, -1] += alpha * steering_activation"
      ],
      "metadata": {
        "id": "tuQSxc_vZ5ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hook functions for different intensities\n",
        "\n",
        "alpha_1 = partial(hook_function, alpha = 1)\n",
        "alpha_2 = partial(hook_function, alpha = 2)\n",
        "alpha_10 = partial(hook_function, alpha = 10)"
      ],
      "metadata": {
        "id": "7wKDC4INldWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steering different layers\n",
        "\n",
        "def steered_layers(original_prompt_tokens, hook_function = hook_function, num_tokens = 5):\n",
        "\n",
        "  prompt_list = []\n",
        "  for layer in range(layers):\n",
        "    print(f'Steering layer {layer}')\n",
        "    current_prompt_tokens = original_prompt_tokens.clone()\n",
        "\n",
        "    for n in range(num_tokens):\n",
        "\n",
        "      logits_steered = model.run_with_hooks(\n",
        "          current_prompt_tokens,\n",
        "          fwd_hooks = [(transformer_lens.utils.get_act_name('resid_post',layer), hook_function)]\n",
        "      )\n",
        "      next_token = logits_steered[:, -1].argmax(dim = -1).unsqueeze(0)\n",
        "      current_prompt_tokens = torch.cat([current_prompt_tokens, next_token], dim = -1)\n",
        "\n",
        "    prompt_list.append(model.to_string(current_prompt_tokens))\n",
        "\n",
        "  return prompt_list"
      ],
      "metadata": {
        "id": "N5lg4Xv6ahEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steering with high intensity\n",
        "\n",
        "results_10 = steered_layers(prompt_tokens, alpha_10)"
      ],
      "metadata": {
        "id": "KIl7Q2ijaqN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results of high intensity steering\n",
        "results_10"
      ],
      "metadata": {
        "id": "vTSoq54Ha1Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Steering with medium intensity\n",
        "\n",
        "results_2 = steered_layers(prompt_tokens, alpha_2)"
      ],
      "metadata": {
        "id": "iZ9QALfmg6xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results of medium intensity\n",
        "results_2"
      ],
      "metadata": {
        "id": "joX77JNDg7NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_z_hook_name(layer):\n",
        "  return utils.get_act_name(\"z\", layer)\n",
        "\n",
        "def generate_all_steering_vectors(model):\n",
        "  \"\"\"\n",
        "  Generates d_model-dimensional (768) steering vectors.\n",
        "  \"\"\"\n",
        "  print(\"Generating steering vectors...\")\n",
        "\n",
        "  fwd_hooks_filter = lambda name: \"attn.hook_z\" in name\n",
        "\n",
        "  _, love_cache = model.run_with_cache(\n",
        "      'Love',\n",
        "      names_filter=fwd_hooks_filter\n",
        "  )\n",
        "  _, hate_cache = model.run_with_cache(\n",
        "      'Hate',\n",
        "      names_filter=fwd_hooks_filter\n",
        "  )\n",
        "\n",
        "  steering_vectors_dict = {}\n",
        "  n_layers = model.cfg.n_layers\n",
        "  n_heads = model.cfg.n_heads\n",
        "\n",
        "  for L_src in range(n_layers):\n",
        "    hook_name_src = get_z_hook_name(L_src)\n",
        "    love_z_activations = love_cache[hook_name_src]\n",
        "    hate_z_activations = hate_cache[hook_name_src]\n",
        "    W_O = model.blocks[L_src].attn.W_O\n",
        "\n",
        "    for H_src in range(n_heads):\n",
        "      love_z_vec = love_z_activations[0, -1, H_src, :] # Shape: [d_head]\n",
        "      hate_z_vec = hate_z_activations[0, -1, H_src, :] # Shape: [d_head]\n",
        "      W_O_h = W_O[H_src] # Shape: [d_head, d_model]\n",
        "\n",
        "\n",
        "      love_d_model_out = torch.einsum(\"d, dm -> m\",\n",
        "                                      love_z_vec, W_O_h)\n",
        "      hate_d_model_out = torch.einsum(\"d, dm -> m\",\n",
        "                                      hate_z_vec, W_O_h)\n",
        "\n",
        "\n",
        "      steering_vec = (love_d_model_out - hate_d_model_out).detach()\n",
        "      steering_vectors_dict[(L_src, H_src)] = steering_vec\n",
        "\n",
        "  print(f\"Done. Generated {len(steering_vectors_dict)} d_model (768) vectors.\")\n",
        "  return steering_vectors_dict"
      ],
      "metadata": {
        "id": "bRspNVSKmLjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get steering vectors for all heads\n",
        "\n",
        "steering_vectors_all_heads = generate_all_steering_vectors(model)"
      ],
      "metadata": {
        "id": "qla5BWsU-GCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steering_vectors_all_heads[(0,0)].shape"
      ],
      "metadata": {
        "id": "m7anKfijB7RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def injection_hook(activation, hook, steering_vector, alpha):\n",
        "  activation [:, -1, :] += alpha * steering_vector\n",
        "  return activation\n",
        "\n",
        "def evaluate_steering_vector(model, prompt_tokens, steering_vector, eval_token_ids, alpha ):\n",
        "  total_scores = 0\n",
        "\n",
        "  for layer in range(model.cfg.n_layers):\n",
        "\n",
        "    hook_function = partial(injection_hook, steering_vector = steering_vector, alpha = alpha)\n",
        "    logits_for_layer = model.run_with_hooks(prompt_tokens, fwd_hooks = [(transformer_lens.utils.get_act_name('resid_post', layer), hook_function)])\n",
        "    logits_softmaxed = logits_for_layer.log_softmax(dim = -1) [:,-1]\n",
        "    love_score = logits_softmaxed[:, eval_token_ids['love']]\n",
        "    hate_score = logits_softmaxed[:, eval_token_ids['hate']]\n",
        "\n",
        "    score = love_score - hate_score\n",
        "    total_scores += score.item()\n",
        "\n",
        "  return total_scores / model.cfg.n_layers"
      ],
      "metadata": {
        "id": "WOYAeVHU-M0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_single_token_id(model, string):\n",
        "\n",
        "  return model.to_tokens(string, prepend_bos=False)[0, 0]\n",
        "\n",
        "eval_token_ids = {\n",
        "    \"love\": get_single_token_id(model, \" love\"),\n",
        "    \"hate\": get_single_token_id(model, \" hate\")\n",
        "}\n",
        "print(f\"Using eval token IDs: {eval_token_ids}\")\n",
        "\n",
        "\n",
        "alpha = 10.0\n",
        "results_source = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=device)\n",
        "\n",
        "print(\"Starting evaluation of 144 source vectors...\")\n",
        "\n",
        "for L_src in range(model.cfg.n_layers):\n",
        "  for H_src in range(model.cfg.n_heads):\n",
        "\n",
        "    current_steering_vec = steering_vectors_all_heads[(L_src, H_src)].to(device)\n",
        "\n",
        "    avg_score = evaluate_steering_vector(\n",
        "        model,\n",
        "        prompt_tokens,\n",
        "        current_steering_vec,\n",
        "        eval_token_ids,\n",
        "        alpha\n",
        "    )\n",
        "\n",
        "    results_source[L_src, H_src] = avg_score\n",
        "\n",
        "  print(f\"  Finished evaluating Layer {L_src}\")\n",
        "\n",
        "print(\"Experiment 1 Complete.\")"
      ],
      "metadata": {
        "id": "MiGhL5sr_DUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_data = results_source.cpu().numpy()\n",
        "\n",
        "fig = px.imshow(\n",
        "    results_data,\n",
        "    title=\"Source Head Scores\",\n",
        "    labels=dict(x=\"Source Head\", y=\"Source Layer\", color=\"Avg Score\"),\n",
        "    x=[str(i) for i in range(model.cfg.n_heads)],\n",
        "    y=[str(i) for i in range(model.cfg.n_layers)],\n",
        "    color_continuous_scale=\"RdBu\",\n",
        "    color_continuous_midpoint=0.0\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"Source Head\",\n",
        "    yaxis_title=\"Source Layer\",\n",
        "    xaxis=dict(side=\"top\"),\n",
        "    yaxis=dict(autorange=\"reversed\")\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "jpHOAbXP_FMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkSgkUBaH0tx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}